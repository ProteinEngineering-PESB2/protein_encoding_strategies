{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProtVec: Amino Acid Embedding Representation of Proteins for Function Classification\n",
    "\n",
    "## Objectives\n",
    "1. Extract features from amino acid sequences for machine learning\n",
    "2. Use features to predict protein family and other structural properties\n",
    "\n",
    "## Abstract\n",
    "This project attempts to reproduce the results from [Asgari 2015](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141287) and to expand it to phage sequences and their protein families. Currently, Asgari's classification of protein families can be reproduced with his using his [trained embedding.](https://github.com/ehsanasgari/Deep-Proteomics). However, his results cannot be reproduced with current attempts to train using the skip-gram negative sampling method detailed in [this tutorial.](http://adventuresinmachinelearning.com/word2vec-keras-tutorial/) Training samples have been attempted with the SwissProt database. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Predicting protein function with machine learning methods require informative features that is extracted from data. A natural language processing (NLP) technique, known as Word2Vec is used to represent a word by its context with a vector that encodes for the probability a context would occur for a word. These vectors are effective at representing meanings of words since words with similar meanings would have similar contexts. For example, the word cat and kitten would have similar contexts that they are used in since they have very similar meanings. These words would thus have very similar vectors. \n",
    "\n",
    "\n",
    "## Methods\n",
    "1. Preprocessing\n",
    "  1. Load dataset containing protein amino acid sequences and Asgari's embedding\n",
    "  2. [Convert sequences to three lists of non-overlapping 3-mer words](https://www.researchgate.net/profile/Mohammad_Mofrad/publication/283644387/figure/fig4/AS:341292040114179@1458381771303/Protein-sequence-splitting-In-order-to-prepare-the-training-data-each-protein-sequence.png) \n",
    "  3. Convert 3-mers to numerical encoding using kmer indicies from Asgari's embedding (row dimension)\n",
    "  4. Generate skipgrams with [Keras function](https://keras.io/preprocessing/sequence/)  \n",
    "        Output: [target word, context word](http://mccormickml.com/assets/word2vec/training_data.png), label  \n",
    "        Label refers to true or false target/context pairing generated for the negative sampling technique             \n",
    "2. Training embedding\n",
    "    1. Create negative sampling skipgram model with Keras [using technique from this tutorial](http://adventuresinmachinelearning.com/word2vec-keras-tutorial/)\n",
    "3. Generate ProtVecs from embedding for a given protein sequence\n",
    "    1. Break protein sequence to list of kmers\n",
    "    2. Convert kmers to vectors by taking the dot product of its one hot vector with the embedding \n",
    "    3. Sum up all vectors for all kmers for a single vector representation for a protein (length 100)        \n",
    "4. Classify protein function with ProtVec features (results currently not working, refer to R script)\n",
    "    1. Use protvecs as training features\n",
    "    2. Use pfam as labels\n",
    "    3. For a given pfam classification, perform binary classification with all of its positive samples and randomly sample an equal amount of negative samples\n",
    "    4. Train SVM model \n",
    "    \n",
    "    \n",
    "## Resources \n",
    "1. Intuition behind Word2Vec http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "2. Tutorial followed for implementation of skip-gram negative sampling (includes code) http://adventuresinmachinelearning.com/word2vec-keras-tutorial/\n",
    "3. Introduction to protein function prediction\n",
    "http://biofunctionprediction.org/cafa-targets/Introduction_to_protein_prediction.pdf\n",
    "\n",
    "## Author\n",
    "Mike Huang  \n",
    "huangjmike@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import skipgrams, pad_sequences, make_sampling_table\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.layers import Embedding, Input, Reshape, Dense, merge\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.manifold import TSNE\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "#Load Ehsan Asgari's embeddings\n",
    "#Source: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141287\n",
    "#Embedding: https://github.com/ehsanasgari/Deep-Proteomics\n",
    "ehsanEmbed =  []\n",
    "with open(\"protVec_100d_3grams.csv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        ehsanEmbed.append(line[0].split('\\t'))\n",
    "threemers = [vec[0] for vec in ehsanEmbed]\n",
    "embeddingMat = [[float(n) for n in vec[1:]] for vec in ehsanEmbed]\n",
    "threemersidx = {} #generate word to index translation dictionary. Use for kmersdict function arguments.\n",
    "for i, kmer in enumerate(threemers):\n",
    "    threemersidx[kmer] = i\n",
    "\n",
    "    \n",
    "#Load NCBI Phage processed dataset - 38420 sequences\n",
    "#table = pd.read_csv(\"90filter.ncbi.statis_phage_gene.csv\", index_col=0)\n",
    "#Remove entries without vector representation due to amino acid sequence not reaching threshold length\n",
    "#table = table[table['Protein'].apply(lambda x: type(x)!=float)]\n",
    "#table[:10]\n",
    "\n",
    "#Load Second NCBI Phage processed dataset - 99520 sequences\n",
    "#cherry = pd.read_csv(\"CherryProteins.csv\")\n",
    "#cherry = cherry.loc[cherry['Component'] != 'HYP'] #Filter hypothetical sequences\n",
    "#cherry  = cherry.loc[cherry['Component'] != 'UNS'] #Filter unsorted sequences\n",
    "#cherryseqs = pd.read_csv(\"cherryaaseqs.csv\")\n",
    "\n",
    "#Load SwissProt 2015 data\n",
    "swissprot = pd.read_csv(\"family_classification_metadata.tab\", sep='\\t')\n",
    "swissprot['Sequence'] = pd.read_csv(\"family_classification_sequences.tab\", sep='\\t')\n",
    "\n",
    "#Create non-redundant, concatenated sequences list between two NCBI datasets for training\n",
    "#seqsunique = [seq[0] for seq in cherryseqs['Protein'].values if seq not in table['Protein'].values]\n",
    "#uniqueinds = [i for i in range(len(cherryseqs)) if cherryseqs['Protein'].iloc[i] not in table['Protein'].values]\n",
    "#cherryunq = cherryseqs.iloc[uniqueinds]\n",
    "#cherryunq = cherryunq.append(table[['Function','Protein']])\n",
    "cherryunq = pd.read_csv(\"cherryall.csv\")\n",
    "\n",
    "#Set parameters\n",
    "vocabsize = len(threemersidx)\n",
    "window_size = 25\n",
    "num_cores = multiprocessing.cpu_count() #For parallel computing\n",
    "\n",
    "#Path to model weights\n",
    "weightspath = '38420sample10000000epochsAsgari.hdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ehsan's embedding trained with SwissProt 2015\n",
    "The embedding has dimensions of 9048 x 100. 9048 represents one for each 3-mer. 100 is the size of the vector representation for each 3-mer. The matrix is a lookup table to get the vector for a given 3-mer. 3-mers not in the table are represented by unk.\n",
    "\n",
    "![](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Let's create the three lists of non-overlapping 3mers as described in the paper.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Mohammad_Mofrad/publication/283644387/figure/fig4/AS:341292040114179@1458381771303/Protein-sequence-splitting-In-order-to-prepare-the-training-data-each-protein-sequence.png\" width=\"40%\">\n",
    "\n",
    "Next, encode each 3-mer to its row index in the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert sequences to three lists of non overlapping 3mers \n",
    "def kmerlists(seq):\n",
    "    kmer0 = []\n",
    "    kmer1 = []\n",
    "    kmer2 = []\n",
    "    for i in range(0,len(seq)-2,3):\n",
    "        if len(seq[i:i+3]) == 3:\n",
    "            kmer0.append(seq[i:i+3])\n",
    "        i+=1\n",
    "        if len(seq[i:i+3]) == 3:\n",
    "            kmer1.append(seq[i:i+3])\n",
    "        i+=1\n",
    "        if len(seq[i:i+3]) == 3:\n",
    "            kmer2.append(seq[i:i+3])\n",
    "    return [kmer0,kmer1,kmer2]\n",
    "\n",
    "#Same as kmerlists function but outputs an index number assigned to each kmer. Index number is from Asgari's embedding\n",
    "def kmersindex(seqs, kmersdict=threemersidx):\n",
    "    kmers = []\n",
    "    for i in range(len(seqs)):\n",
    "        kmers.append(kmerlists(seqs[i]))\n",
    "    kmers = np.array(kmers).flatten().flatten(order='F')\n",
    "    kmersindex = []\n",
    "    for seq in kmers:\n",
    "        temp = []\n",
    "        for kmer in seq:\n",
    "            try:\n",
    "                temp.append(kmersdict[kmer])\n",
    "            except:\n",
    "                temp.append(kmersdict['<unk>'])\n",
    "        kmersindex.append(temp)\n",
    "    return kmersindex\n",
    "\n",
    "sampling_table = make_sampling_table(vocabsize)\n",
    "def generateskipgramshelper(kmersindicies): \n",
    "    couples, labels = skipgrams(kmersindicies, vocabsize, window_size=window_size, sampling_table=sampling_table)\n",
    "    if len(couples)==0: \n",
    "        couples, labels = skipgrams(kmersindicies, vocabsize, window_size=window_size, sampling_table=sampling_table)\n",
    "    if len(couples)==0:\n",
    "        couples, labels = skipgrams(kmersindicies, vocabsize, window_size=window_size, sampling_table=sampling_table)\n",
    "    else:\n",
    "        word_target, word_context = zip(*couples)\n",
    "        return word_target, word_context, labels\n",
    "    \n",
    "def generateskipgrams(seqs,kmersdict=threemersidx):\n",
    "    kmersidx = kmersindex(seqs,kmersdict)\n",
    "    return Parallel(n_jobs=num_cores)(delayed(generateskipgramshelper)(kmers) for kmers in kmersidx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sequence\n",
      "MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL\n",
      "\n",
      "Convert sequence to list of kmers\n",
      "[['MAF', 'SAE', 'DVL', 'KEY', 'DRR', 'RRM', 'EAL', 'LLS', 'LYY', 'PND', 'RKL', 'LDY', 'KEW', 'SPP', 'RVQ', 'VEC', 'PKA', 'PVE', 'WNN', 'PPS', 'EKG', 'LIV', 'GHF', 'SGI', 'KYK', 'GEK', 'AQA', 'SEV', 'DVN', 'KMC', 'CWV', 'SKF', 'KDA', 'MRR', 'YQG', 'IQT', 'CKI', 'PGK', 'VLS', 'DLD', 'AKI', 'KAY', 'NLT', 'VEG', 'VEG', 'FVR', 'YSR', 'VTK', 'QHV', 'AAF', 'LKE', 'LRH', 'SKQ', 'YEN', 'VNL', 'IHY', 'ILT', 'DKR', 'VDI', 'QHL', 'EKD', 'LVK', 'DFK', 'ALV', 'ESA', 'HRM', 'RQG', 'HMI', 'NVK', 'YIL', 'YQL', 'LKK', 'HGH', 'GPD', 'GPD', 'ILT', 'VKT', 'GSK', 'GVL', 'YDD', 'SFR', 'KIY', 'TDL', 'GWK', 'FTP'], ['AFS', 'AED', 'VLK', 'EYD', 'RRR', 'RME', 'ALL', 'LSL', 'YYP', 'NDR', 'KLL', 'DYK', 'EWS', 'PPR', 'VQV', 'ECP', 'KAP', 'VEW', 'NNP', 'PSE', 'KGL', 'IVG', 'HFS', 'GIK', 'YKG', 'EKA', 'QAS', 'EVD', 'VNK', 'MCC', 'WVS', 'KFK', 'DAM', 'RRY', 'QGI', 'QTC', 'KIP', 'GKV', 'LSD', 'LDA', 'KIK', 'AYN', 'LTV', 'EGV', 'EGF', 'VRY', 'SRV', 'TKQ', 'HVA', 'AFL', 'KEL', 'RHS', 'KQY', 'ENV', 'NLI', 'HYI', 'LTD', 'KRV', 'DIQ', 'HLE', 'KDL', 'VKD', 'FKA', 'LVE', 'SAH', 'RMR', 'QGH', 'MIN', 'VKY', 'ILY', 'QLL', 'KKH', 'GHG', 'PDG', 'PDI', 'LTV', 'KTG', 'SKG', 'VLY', 'DDS', 'FRK', 'IYT', 'DLG', 'WKF', 'TPL'], ['FSA', 'EDV', 'LKE', 'YDR', 'RRR', 'MEA', 'LLL', 'SLY', 'YPN', 'DRK', 'LLD', 'YKE', 'WSP', 'PRV', 'QVE', 'CPK', 'APV', 'EWN', 'NPP', 'SEK', 'GLI', 'VGH', 'FSG', 'IKY', 'KGE', 'KAQ', 'ASE', 'VDV', 'NKM', 'CCW', 'VSK', 'FKD', 'AMR', 'RYQ', 'GIQ', 'TCK', 'IPG', 'KVL', 'SDL', 'DAK', 'IKA', 'YNL', 'TVE', 'GVE', 'GFV', 'RYS', 'RVT', 'KQH', 'VAA', 'FLK', 'ELR', 'HSK', 'QYE', 'NVN', 'LIH', 'YIL', 'TDK', 'RVD', 'IQH', 'LEK', 'DLV', 'KDF', 'KAL', 'VES', 'AHR', 'MRQ', 'GHM', 'INV', 'KYI', 'LYQ', 'LLK', 'KHG', 'HGP', 'DGP', 'DIL', 'TVK', 'TGS', 'KGV', 'LYD', 'DSF', 'RKI', 'YTD', 'LGW', 'KFT']]\n",
      "\n",
      "Convert kmers to their index on the embedding\n",
      "[[4330, 704, 165, 2795, 2594, 4177, 9, 12, 4155, 4300, 467, 2012, 6034, 1854, 3001, 5719, 2112, 1382, 7163, 1380, 756, 593, 4582, 718, 3482, 648, 291, 956, 2337, 7690, 7833, 3151, 986, 4003, 4117, 3390, 6159, 1915, 128, 575, 941, 2787, 1260, 507, 507, 2641, 3455, 1665, 5098, 792, 49, 2474, 2708, 4170, 1220, 6212, 566, 2977, 1079, 3490, 1401, 294, 2997, 45, 1012, 6887, 2326, 7200, 2252, 2647, 3514, 105, 5221, 2962, 2962, 566, 1453, 1437, 126, 3808, 2895, 3362, 890, 5668, 3102], [1268, 892, 354, 3434, 376, 4614, 5, 24, 6082, 4205, 59, 4097, 6338, 2660, 1934, 6388, 2277, 5623, 3420, 1820, 410, 470, 4729, 1038, 3533, 220, 1937, 1057, 2191, 7960, 5572, 2975, 3608, 3041, 2208, 7016, 2430, 448, 243, 112, 954, 4503, 428, 579, 2026, 3606, 1337, 3779, 3357, 696, 83, 4422, 4470, 1414, 1146, 6171, 537, 1189, 2959, 2485, 411, 1088, 2518, 108, 3879, 4619, 4982, 5097, 3870, 2748, 167, 3990, 3140, 1281, 2442, 428, 1225, 1388, 2257, 1980, 3114, 4007, 456, 7310, 750], [1528, 687, 49, 4107, 376, 3070, 2, 1977, 5138, 2820, 33, 3143, 5868, 2353, 1907, 6220, 857, 6915, 4110, 1134, 304, 3648, 1332, 3972, 1005, 1789, 682, 641, 5164, 7966, 1153, 2533, 3127, 4646, 2862, 6234, 1458, 202, 431, 1464, 722, 3510, 699, 499, 1576, 3743, 1540, 5448, 17, 1158, 278, 4846, 5132, 3133, 3189, 2647, 2383, 1905, 5440, 48, 211, 2461, 73, 1006, 3681, 5038, 6485, 2575, 3252, 3153, 38, 3956, 5005, 2828, 325, 1188, 571, 842, 2324, 2671, 1478, 4128, 4201, 3964]]\n",
      "\n",
      "Sample skipgram input:\n",
      "Word Target: 986\n",
      "Word Context: 12\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample sequence\")\n",
    "print(swissprot['Sequence'].iloc[0])\n",
    "print(\"\")\n",
    "print(\"Convert sequence to list of kmers\")\n",
    "print(kmerlists(swissprot['Sequence'].iloc[0]))\n",
    "print(\"\")\n",
    "print(\"Convert kmers to their index on the embedding\")\n",
    "print(kmersindex(swissprot['Sequence'].iloc[:1]))\n",
    "print(\"\")\n",
    "testskipgrams = generateskipgrams(swissprot['Sequence'].iloc[:1])\n",
    "print(\"Sample skipgram input:\")\n",
    "print(\"Word Target:\", testskipgrams[0][0][0])\n",
    "print(\"Word Context:\", testskipgrams[0][1][0])\n",
    "print(\"Label:\", testskipgrams[0][2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 1, 100)        904800      input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 100, 1)        0           embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 100, 1)        0           embedding[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 1, 1)          0           reshape_1[0][0]                  \n",
      "                                                                   reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 1)             0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             2           reshape_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 904,802\n",
      "Trainable params: 904,802\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  from ipykernel import kernelapp as app\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"me...)`\n"
     ]
    }
   ],
   "source": [
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "vector_dim = 100\n",
    "\n",
    "embedding = Embedding(vocabsize, vector_dim, input_length=1, name='embedding')\n",
    "embedding.build((None,))\n",
    "embedding.set_weights(np.array([embeddingMat])) #Load Asgari's embedding as initial weights\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# create the primary training model\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(input=[input_target, input_context], output=similarity)\n",
    "\n",
    "#model.load_weights(weightspath)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_dictionary = threemers\n",
    "vocab_size = vocabsize\n",
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        for i in range(vocab_size):\n",
    "            in_arr1[0,] = valid_word_idx\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training embedding with SwissProt 2015, sequential sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "samplesize = len(swissprot)\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "chunklength = 1000\n",
    "#ite=0\n",
    "losses = 0\n",
    "#print(\"Loading part\",ite+1,\"/\",int(samplesize/chunklength), \"of data, length of \",samplesize)\n",
    "#kmerskipgrams=generateskipgrams(swissprot['Sequence'].iloc[:chunklength-1].values,threemersidx)\n",
    "\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for cnt in range(samplesize):\n",
    "        idx = cnt-chunklength*ite\n",
    "        if type(kmerskipgrams[idx]) == tuple:\n",
    "            for idx2 in range(len(kmerskipgrams[idx][0])):\n",
    "                arr_1[0,] = kmerskipgrams[idx][0][idx2]\n",
    "                arr_2[0,] = kmerskipgrams[idx][1][idx2]\n",
    "                arr_3[0,] = kmerskipgrams[idx][2][idx2]\n",
    "                loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "                #losses += loss\n",
    "                if idx2 % 1000 == 0:\n",
    "                    #print(\"Iteration {}, loss={}\".format(cnt, loss), \"Average loss in last 1000 samples\", losses/1000)\n",
    "                    print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "                    losses = 0\n",
    "        if cnt % 1000 == 0:\n",
    "            sim_cb.run_sim()\n",
    "        if cnt % chunklength == 0 and cnt != 0:\n",
    "            ite+=1\n",
    "            print(\"Loading \",ite+1,\"/\",int(samplesize/chunklength),\"part of data\")\n",
    "            del kmerskipgrams\n",
    "            kmerskipgrams=generateskipgrams(swissprot['Sequence'].iloc[ite*chunklength:(ite+1)*chunklength-1].values,threemersidx)    \n",
    "        if cnt % samplesize/2 == 0:\n",
    "            model.save_weights('swissprothalf.hdf')\n",
    "model.save_weights('swissprot.hdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embeddingweights = model.layers[2].get_weights()[0]\n",
    "\n",
    "def protvec(kmersdict, seq, embeddingweights=embeddingMat):\n",
    "    #Convert seq to three lists of kmers \n",
    "    kmerlist = kmerlists(seq) \n",
    "    kmerlist = [j for i in kmerlist for j in i]\n",
    "    #Convert center kmers to their vector representations\n",
    "    kmersvec = [0]*100\n",
    "    for kmer in kmerlist:\n",
    "        try:\n",
    "            kmersvec = np.add(kmersvec,embeddingweights[kmersdict[kmer]])\n",
    "        except:\n",
    "            kmersvec = np.add(kmersvec,embeddingweights[kmersdict['<unk>']])\n",
    "    return kmersvec\n",
    "\n",
    "def formatprotvecs(protvecs):\n",
    "    #Format protvecs for classifier inputs by transposing the matrix\n",
    "    protfeatures = []\n",
    "    for i in range(100):\n",
    "        protfeatures.append([vec[i] for vec in protvecs])\n",
    "    protfeatures = np.array(protfeatures).reshape(len(protvecs),len(protfeatures))\n",
    "    return protfeatures\n",
    "\n",
    "def formatprotvecsnormalized(protvecs):\n",
    "    #Formatted protvecs with feature normalization\n",
    "    protfeatures = []\n",
    "    for i in range(100):\n",
    "        tempvec = [vec[i] for vec in protvecs]\n",
    "        mean = np.mean(tempvec)\n",
    "        var = np.var(tempvec)\n",
    "        protfeatures.append([(vec[i]-mean)/var for vec in protvecs])\n",
    "    protfeatures = np.array(protfeatures).reshape(len(protvecs),len(protfeatures))\n",
    "    return protfeatures\n",
    "\n",
    "def sequences2protvecsCSV(filename, seqs, kmersdict=threemersidx, embeddingweights=embeddingMat):\n",
    "    #Convert a list of sequences to protvecs and save protvecs to a csv file\n",
    "    #ARGUMENTS;\n",
    "    #filename: string, name of csv file to save to, i.e. \"sampleprotvecs.csv\"\n",
    "    #seqs: list, list of amino acid sequences\n",
    "    #kmersdict: dict to look up index of kmer on embedding, default: Asgari's embedding index\n",
    "    #embeddingweights: 2D list or np.array, embedding vectors, default: Asgari's embedding vectors\n",
    "\n",
    "    swissprotvecs = Parallel(n_jobs=num_cores)(delayed(protvec)(kmersdict, seq, embeddingweights) for seq in seqs)\n",
    "    swissprotvecsdf = pd.DataFrame(formatprotvecs(swissprotvecs))\n",
    "    swissprotvecsdf.to_csv(filename, index=False)\n",
    "    return swissprotvecsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-15.906601</td>\n",
       "      <td>-15.786575</td>\n",
       "      <td>-12.858975</td>\n",
       "      <td>-20.429255</td>\n",
       "      <td>-25.811314</td>\n",
       "      <td>-1.425858</td>\n",
       "      <td>-0.273261</td>\n",
       "      <td>1.218515</td>\n",
       "      <td>-2.768131</td>\n",
       "      <td>-1.975148</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.353896</td>\n",
       "      <td>1.683457</td>\n",
       "      <td>-2.855346</td>\n",
       "      <td>-4.200350</td>\n",
       "      <td>-6.863898</td>\n",
       "      <td>-8.975450</td>\n",
       "      <td>0.160262</td>\n",
       "      <td>-3.139883</td>\n",
       "      <td>-12.887104</td>\n",
       "      <td>-15.625758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.501441</td>\n",
       "      <td>-2.576011</td>\n",
       "      <td>0.572682</td>\n",
       "      <td>3.017890</td>\n",
       "      <td>3.397684</td>\n",
       "      <td>-8.050783</td>\n",
       "      <td>-7.461454</td>\n",
       "      <td>-4.145822</td>\n",
       "      <td>-1.219607</td>\n",
       "      <td>-9.140709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279602</td>\n",
       "      <td>4.074085</td>\n",
       "      <td>3.937202</td>\n",
       "      <td>-5.439727</td>\n",
       "      <td>0.134528</td>\n",
       "      <td>-1.727660</td>\n",
       "      <td>-6.800861</td>\n",
       "      <td>-4.274481</td>\n",
       "      <td>-0.687948</td>\n",
       "      <td>-6.683564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.571641</td>\n",
       "      <td>6.087940</td>\n",
       "      <td>-0.093153</td>\n",
       "      <td>1.788396</td>\n",
       "      <td>3.143507</td>\n",
       "      <td>0.587536</td>\n",
       "      <td>4.343174</td>\n",
       "      <td>3.281648</td>\n",
       "      <td>4.594139</td>\n",
       "      <td>6.544705</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.428951</td>\n",
       "      <td>-8.151144</td>\n",
       "      <td>-4.075138</td>\n",
       "      <td>-3.999045</td>\n",
       "      <td>-6.826098</td>\n",
       "      <td>-19.353062</td>\n",
       "      <td>-13.091738</td>\n",
       "      <td>-14.026761</td>\n",
       "      <td>-31.865056</td>\n",
       "      <td>-33.461093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.186221</td>\n",
       "      <td>15.031310</td>\n",
       "      <td>4.099640</td>\n",
       "      <td>-7.429306</td>\n",
       "      <td>0.673389</td>\n",
       "      <td>12.435832</td>\n",
       "      <td>28.053988</td>\n",
       "      <td>13.094559</td>\n",
       "      <td>18.319831</td>\n",
       "      <td>23.192327</td>\n",
       "      <td>...</td>\n",
       "      <td>3.338261</td>\n",
       "      <td>7.016894</td>\n",
       "      <td>-0.624116</td>\n",
       "      <td>0.052345</td>\n",
       "      <td>3.364417</td>\n",
       "      <td>-5.378887</td>\n",
       "      <td>-2.588111</td>\n",
       "      <td>-7.852132</td>\n",
       "      <td>-9.400000</td>\n",
       "      <td>-9.117914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-11.274578</td>\n",
       "      <td>-12.735979</td>\n",
       "      <td>-9.380683</td>\n",
       "      <td>-24.705562</td>\n",
       "      <td>-24.733077</td>\n",
       "      <td>-0.129001</td>\n",
       "      <td>-6.032588</td>\n",
       "      <td>2.454767</td>\n",
       "      <td>6.513942</td>\n",
       "      <td>1.185852</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.999941</td>\n",
       "      <td>-8.337236</td>\n",
       "      <td>-1.482302</td>\n",
       "      <td>2.712613</td>\n",
       "      <td>-1.113474</td>\n",
       "      <td>18.634332</td>\n",
       "      <td>28.020647</td>\n",
       "      <td>13.635002</td>\n",
       "      <td>25.419577</td>\n",
       "      <td>33.273547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3          4          5   \\\n",
       "0 -15.906601 -15.786575 -12.858975 -20.429255 -25.811314  -1.425858   \n",
       "1  -1.501441  -2.576011   0.572682   3.017890   3.397684  -8.050783   \n",
       "2   4.571641   6.087940  -0.093153   1.788396   3.143507   0.587536   \n",
       "3   1.186221  15.031310   4.099640  -7.429306   0.673389  12.435832   \n",
       "4 -11.274578 -12.735979  -9.380683 -24.705562 -24.733077  -0.129001   \n",
       "\n",
       "          6          7          8          9     ...            90        91  \\\n",
       "0  -0.273261   1.218515  -2.768131  -1.975148    ...     -2.353896  1.683457   \n",
       "1  -7.461454  -4.145822  -1.219607  -9.140709    ...      0.279602  4.074085   \n",
       "2   4.343174   3.281648   4.594139   6.544705    ...     -5.428951 -8.151144   \n",
       "3  28.053988  13.094559  18.319831  23.192327    ...      3.338261  7.016894   \n",
       "4  -6.032588   2.454767   6.513942   1.185852    ...     -4.999941 -8.337236   \n",
       "\n",
       "         92        93        94         95         96         97         98  \\\n",
       "0 -2.855346 -4.200350 -6.863898  -8.975450   0.160262  -3.139883 -12.887104   \n",
       "1  3.937202 -5.439727  0.134528  -1.727660  -6.800861  -4.274481  -0.687948   \n",
       "2 -4.075138 -3.999045 -6.826098 -19.353062 -13.091738 -14.026761 -31.865056   \n",
       "3 -0.624116  0.052345  3.364417  -5.378887  -2.588111  -7.852132  -9.400000   \n",
       "4 -1.482302  2.712613 -1.113474  18.634332  28.020647  13.635002  25.419577   \n",
       "\n",
       "          99  \n",
       "0 -15.625758  \n",
       "1  -6.683564  \n",
       "2 -33.461093  \n",
       "3  -9.117914  \n",
       "4  33.273547  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences2protvecsCSV(\"testprotvecs.csv\", swissprot['Sequence'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Protein Function Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report,log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.ndimage.measurements import center_of_mass, label\n",
    "from skimage.measure import regionprops\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from scipy.stats import percentileofscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "binlab=lb.fit_transform(table['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits=10\n",
    "kfold=StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "models=[RandomForestClassifier(),\n",
    "        GradientBoostingClassifier(),]\n",
    "name=[\"Random Forest\", \"Gradient Boosting\"]\n",
    "\n",
    "predictedmodels={}\n",
    "\n",
    "for nm, clf in zip(name[:-1], models[:-1]):\n",
    "    print(nm)\n",
    "    predicted=[]\n",
    "    for train,test in kfold.split(featuretable,binlab[:,0]):\n",
    "    scores=cross_val_score(clf,featuretable, binlab, cv=StratifiedKFold(n_splits=n_splits, shuffle=True), n_jobs=-1, scoring='neg_log_loss')\n",
    "    print(\"Cross-validated logloss\",-np.mean(scores))\n",
    "    print(\"---------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(featuretable, binlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuretable = pd.DataFrame(np.array(protfeatures).reshape(9173,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Binary classification of single function\n",
    "intsamples = table.loc[table['labels']=='INF']\n",
    "intsamples['binarylabel'] = [1]*len(intsamples)\n",
    "nonint = table.loc[table['labels'] != 'INF']\n",
    "nonint['binarylabel'] = [0]*len(nonint)\n",
    "intsamples = intsamples.append(nonint.sample(frac=len(intsamples)/len(nonint)))\n",
    "intsamples = intsamples.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = formatprotvecs(intsamples['ProtVecs'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models=[LogisticRegression(C=0.1),\n",
    "        RandomForestClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "    SVC(C=0.02,kernel='rbf', probability=True)]\n",
    "name=[\"Logistic Regression\",\"Random Forest\", \"Gradient Boosting\",\"SVM rbf kernel\"]\n",
    "\n",
    "predictedmodels={}\n",
    "\n",
    "for nm, clf in zip(name, models):\n",
    "    print(nm)\n",
    "    scores=cross_val_score(clf,features, intsamples['binarylabel'], cv=StratifiedKFold(n_splits=n_splits, shuffle=True), n_jobs=-1, scoring='neg_log_loss')\n",
    "    print(\"Cross-validated logloss\",-np.mean(scores))\n",
    "    print(\"---------------------------------------\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def BinaryClassification(x,y):\n",
    "    n_splits=10\n",
    "    kfold=StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    models=[LogisticRegression(C=0.1),\n",
    "            RandomForestClassifier(),\n",
    "            GradientBoostingClassifier(),\n",
    "        SVC(C=1,kernel='rbf')]\n",
    "    name=[\"Logistic Regression\", \"Random Forest\", \"Gradient Boosting\", \"SVM with rbf kernel\"]\n",
    "\n",
    "    predictedmodels={}\n",
    "\n",
    "    for nm, clf in zip(name[-1:], models[-1:]):\n",
    "        print(nm)\n",
    "        predicted=[]\n",
    "        labelcv=[]\n",
    "        for train,test in kfold.split(x, y):\n",
    "            clf.fit(x[train],y[train])\n",
    "            predicted.append(clf.predict(x[test]))\n",
    "            labelcv.append(y[test])\n",
    "        #scores=cross_val_score(clf,x, y, cv=StratifiedKFold(n_splits=n_splits, shuffle=True), n_jobs=-1, scoring='neg_log_loss')\n",
    "        predicted=np.concatenate(np.array(predicted),axis=0)\n",
    "        labelcv=np.concatenate(np.array(labelcv),axis=0)\n",
    "        predictedmodels[nm]=predicted\n",
    "        #roc=roc_curve(labelcv,predicted)\n",
    "        #print(\"Average precision score:\", average_precision_score(labelcv,predicted))\n",
    "        #print(\"Area under curve:\", auc(roc[0],roc[1]))\n",
    "        #plt.plot(roc[0],roc[1])\n",
    "        #print(-scores)\n",
    "        print(classification_report(labelcv,predicted))\n",
    "        print(confusion_matrix(labelcv,predicted))\n",
    "        print(\"Cross-validated logloss\",-np.mean(scores))\n",
    "        print(\"---------------------------------------\")\n",
    "        #plt.plot(rocrandom[0],rocrandom[1])\n",
    "    #plt.title('ROC')\n",
    "    #plt.ylabel('TPrate')\n",
    "    #plt.xlabel('FPrate')\n",
    "    #plt.legend(name)\n",
    "    #plt.savefig(\"clfroccomparison.png\",dpi=300)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up accuracy, sensitivty, specificity as evaluation scorews\n",
    "#Normalize features\n",
    "#Try Asgari 2015 initial weights\n",
    "#Try regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate Asgari 2015 Protein family classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swissprot = pd.read_csv(\"family_classification_metadata.tab\", sep='\\t')\n",
    "swissprot['Sequence'] = pd.read_csv(\"family_classification_sequences.tab\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swissprot.loc[swissprot['FamilyDescription'] == '50S ribosome-binding GTPase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del swissprotseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concordance = [table.iloc[i] for i in range(len(table)) if table['Protein'].iloc[i] in swissprot['Sequence'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=num_cores)(delayed(generateskipgrams)(kmers) for kmers in kmersindex[20000:])\n",
    "word_target = []\n",
    "word_context = []\n",
    "labels = []\n",
    "for sample in results:\n",
    "    if type(sample) == tuple:\n",
    "        word_target += sample[0]\n",
    "        word_context += sample[1]\n",
    "        labels += sample[2]\n",
    "del results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sample 50S ribosome-binding GTPase and equal amount of negative cases\n",
    "def SampleBinaryClassification(table, function,ProtVecs):\n",
    "    pos = table.loc[table['FamilyDescription'] == function]\n",
    "    neg = table.loc[table['FamilyDescription'] != function]\n",
    "    pos['binarylabel'] = np.ones(len(pos), dtype=bool)\n",
    "    neg = neg.sample(frac=len(pos)/len(neg))\n",
    "    neg['binarylabel'] = np.zeros(len(neg), dtype=bool)\n",
    "    pos = pos.append(neg)\n",
    "    pos = pos.sample(frac=1)\n",
    "    #print(\"Generating ProtVecs\")\n",
    "    #ProtVecs = Parallel(n_jobs=num_cores)(delayed(protvec)(len(threemers), threemersidx, seq, embeddingMat) for seq in pos['Sequence'])\n",
    "    #pos['ProtVecs'] = ProtVecs\n",
    "    features = formatprotvecs(ProtVecs)\n",
    "    BinaryClassification(features,pfambinary['binarylabel'].values)\n",
    "    return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ProtVecs = Parallel(n_jobs=num_cores)(delayed(protvec)(threemersidx, seq, embeddingMat) for seq in pfambinary['Sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SampleBinaryClassification(swissprot,'50S ribosome-binding GTPase',famclass.iloc[pfambinary.index].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = formatprotvecsnormalized(famclass.iloc[pfambinary.index].values)\n",
    "#labels = LabelBinarizer().fit_transform(pfambinary['binarylabel'].values)\n",
    "BinaryClassification(features,pfambinary['binarylabel'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = formatprotvecsnormalized(famclass.iloc[pfambinary.index].values)\n",
    "#labels = LabelBinarizer().fit_transform(pfambinary['binarylabel'].values)\n",
    "BinaryClassification(features,pfambinary['binarylabel'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pfambinary['binarylabel'].values\n",
    "def fit_model(X, y, clf):\n",
    "    cv_sets = ShuffleSplit(X.shape[0], n_iter = 5, test_size = 0.20, random_state = 42)\n",
    "    params = {'C':np.arange(10,100),\n",
    "             'gamma':np.arange(1e-2,1e-1)}\n",
    "    grid = GridSearchCV(clf, params, cv=cv_sets, n_jobs=-1)\n",
    "    grid = grid.fit(X, y)\n",
    "    return grid.best_params_, grid.best_score_, grid.best_estimator_\n",
    "\n",
    "best_params, best_score, optimal_svm=fit_model(features,labels,SVC())\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (best_params, best_score))\n",
    "print(optimal_svm)\n",
    "\n",
    "name=[\"Optimized SVM\"]\n",
    "print(name)\n",
    "#scores=cross_val_score(optimal_gb,inputfeatures[featurelist], malignantlabel, cv=5, scoring='neg_log_loss')\n",
    "#print(-scores)\n",
    "#print(\"Cross-validated logloss\",-np.mean(scores))\n",
    "print(\"---------------------------------------\")\n",
    "clf=optimal_svm\n",
    "clf.fit(features[train],labels[train])\n",
    "print(classification_report(labels[test],clf.predict(features[test])))\n",
    "print(confusion_matrix(labels[test],clf.predict(features[test])))\n",
    "#roc=roc_curve(Ytest,clf.predict_proba(Xtest[featurelist])[:,1])\n",
    "#print(clf.feature_importances)\n",
    "#ROC curve\n",
    "#plt.plot(roc[0],roc[1], alpha=0.5)\n",
    "#plt.plot(rocrandom[0],rocrandom[1])\n",
    "\n",
    "#scores=cross_val_score(GradientBoostingClassifier(),inputfeatures[featurelist], malignantlabel, cv=5, scoring='neg_log_loss')\n",
    "#print(classification_report(Ytest,model.predict(Xtest[featurelist])))\n",
    "#print(-scores)\n",
    "#print(\"Cross-validated logloss\",-np.mean(scores))\n",
    "#print(\"---------------------------------------\")\n",
    "#clf=SVC()\n",
    "#clf.fit(Xtrain[featurelist],Ytrain)\n",
    "#roc=roc_curve(Ytest,clf.predict_proba(Xtest[featurelist])[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Collect cherry annotated dataset\n",
    "#Import into R and use bioconductor to translate DNAseqs to AASeqs\n",
    "#Get protvecs for each AAseq\n",
    "#Load into classifier to determine prediction rate for each category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
